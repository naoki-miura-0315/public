{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec06b7d-bc53-4d40-b2bc-41681c947d15",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換\n",
    "\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfceb68c-cadd-46c0-8522-554aa38c8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import ast\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "25a506f9-313f-42aa-85e9-c845441d6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  単語とidの辞書を作成\n",
    "\n",
    "word_2_id = {}\n",
    "words = []\n",
    "\n",
    "with open('data/train.txt') as fi:\n",
    "    for line in fi:\n",
    "        sent = line.split('\\t')[0]\n",
    "        sent = sent.replace(\"'\",\"\").replace('\"','').replace('.','').replace('?','')\n",
    "        for word in sent.split():\n",
    "            words.append(word)#  出現する単語をとりあえず入れとく\n",
    "        \n",
    "#  単語の数を数える\n",
    "word_count = Counter(words) \n",
    "#  出現回数順にソート\n",
    "sort_word_count = word_count.most_common()\n",
    "#print(sort_word_count)\n",
    "#  一応保存\n",
    "with open('work/word_count.pickle', mode='wb') as f:\n",
    "    pickle.dump(sort_word_count, f)\n",
    "\n",
    "\n",
    "#  idを付与\n",
    "for i, w_and_c in enumerate(sort_word_count):\n",
    "    word = w_and_c[0]\n",
    "    count = w_and_c[1]\n",
    "    if count > 1:\n",
    "        word_2_id[word] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0da4b4c3-085f-40c2-9d61-75025c26c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  word_2_idを利用してidを付与する関数\n",
    "def text_2_id_list(text):\n",
    "    sentence_id_list = []\n",
    "    words = text.replace(\"'\",\"\").replace('\"','').replace('.','').replace('?','')\n",
    "    words = words.rstrip().split()\n",
    "    for word in words:\n",
    "        if word in word_2_id:\n",
    "            word_id = word_2_id[word]\n",
    "            sentence_id_list.append(word_id)\n",
    "        else:\n",
    "            sentence_id_list.append(int(0))\n",
    "            \n",
    "    return sentence_id_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "id": "bfcb1d79-3d3e-4784-891f-8e1eb0d52bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 2002, 1, 1533, 1, 301]"
      ]
     },
     "execution_count": 1170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'I want to go to Japan.'\n",
    "text_2_id_list(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "id": "697d000f-cbbd-4f58-a5b3-b2fa0015d039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10137\n",
      "10137\n"
     ]
    }
   ],
   "source": [
    "print(len(word_2_id))\n",
    "print(len(word_3_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "id": "5b476f6d-aa05-4c33-912e-bbaa76bd9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_3_id = {}\n",
    "myfile = open('work/word_count.pickle', 'rb')\n",
    "rmydata = pickle.load(myfile)\n",
    "for i, w_and_c in enumerate(rmydata):\n",
    "    word = w_and_c[0]\n",
    "    \n",
    "    count = w_and_c[1]\n",
    "    if count > 1:\n",
    "        word_3_id[word] = i + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90138d3c-7c89-4cb6-8b69-5f2daf816985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "'''GPUチェック'''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "id": "5f33665f-1206-404b-98ed-d73758a3fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパラたち\n",
    "\n",
    "#  単語埋め込み次元数\n",
    "dw = 300\n",
    "#  隠れ状態ベクトルの次元数\n",
    "dh = 50\n",
    "\n",
    "input_size = len(words) + 2 #語彙サイズ＋１尚且つ０のやつもあるので＋１\n",
    "padding_idx = 0\n",
    "emb_size = dw\n",
    "hidden_size = dh\n",
    "output_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "id": "d9c7432f-19e0-4935-8195-b0c4448d6f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(109762, 300, padding_idx=109761)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "id": "68b85b6e-3138-4f43-b9c5-1d9593348ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNNを用いたモデル定義を行う\n",
    "\n",
    "class My_rnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #  埋め込みの定義\n",
    "        self.emb = nn.Embedding(input_size, emb_size, padding_idx)\n",
    "        #  RNNの定義\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)#batch_size, seq_len, dim\n",
    "        #  出力層の定義\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        #  softmax層\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    #  予測関数の定義    \n",
    "    def forward(self, x, h0=None):\n",
    "        x = self.emb(x)#  埋め込み層\n",
    "        y, h = self.rnn(x, h0)#  xと初期の隠れ状態h0でRNNモデルを実行\n",
    "        y = y[:, -1, :]#  時間経過の中で最後のステップを取得  #系列長、バッチサイズ、次元数\n",
    "        y = self.fc(y)#  全結合層 →　４次元へ\n",
    "        #y = self.softmax(y, dim=1)#  ４次元を確率へ\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "id": "0c5a6873-f41a-46c8-be0c-4b98cecda3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(109762, 300, padding_idx=109761)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d0081bb-589c-403f-a669-4b4ec8a8bc2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_2_id_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI want to go to Japan.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mtext_2_id_list\u001b[49m(sample), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_2_id_list' is not defined"
     ]
    }
   ],
   "source": [
    "sample = 'I want to go to Japan.'\n",
    "\n",
    "x = torch.tensor(text_2_id_list(sample), dtype=torch.int64)\n",
    "print(x)\n",
    "print(x.size())\n",
    "print(model(x))\n",
    "print(F.softmax(model(x), dim=-1))\n",
    "print(torch.argmax(model(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4ed2d-e373-4be7-9faa-7009c7f4beb0",
   "metadata": {},
   "source": [
    "#  82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "id": "c987df5a-78f4-4b39-971a-a13b334270fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "9a06e93a-c553-4aff-be98-cc24d682677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパラたち\n",
    "\n",
    "#  単語埋め込み次元数\n",
    "dw = 300\n",
    "#  隠れ状態ベクトルの次元数\n",
    "dh = 50\n",
    "\n",
    "input_size = len(words) + 2 #語彙サイズ＋１尚且つ０のやつもあるので＋１,\"-1\"の分で＋１\n",
    "padding_idx = len(words) +1\n",
    "emb_size = dw\n",
    "hidden_size = dh\n",
    "output_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ad63549-07e0-42ca-b21f-e1f5146d3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパラたち\n",
    "\n",
    "#  単語埋め込み次元数\n",
    "dw = 300\n",
    "#  隠れ状態ベクトルの次元数\n",
    "dh = 50\n",
    "\n",
    "input_size = len(word_2_id.values()) + 2 #語彙サイズ＋１尚且つ０のやつもあるので＋１,\"-1\"の分で＋１\n",
    "padding_idx = len(word_2_id.values()) +1\n",
    "emb_size = dw\n",
    "hidden_size = dh\n",
    "output_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "id": "12fe07e5-c18b-45b4-975f-92df9f8f074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10139\n",
      "10138\n"
     ]
    }
   ],
   "source": [
    "print(input_size)\n",
    "print(padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "id": "a5f9fc8f-ae2f-47ce-88e2-e4242bd1f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_rnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #  埋め込みの定義\n",
    "        self.emb = nn.Embedding(input_size, emb_size, padding_idx)\n",
    "        #  RNNの定義\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size,batch_first=True)#batch_size, seq_len, dim\n",
    "        #  出力層の定義\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        #  softmax層\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    #  予測関数の定義    \n",
    "    def forward(self, x, h0=None):\n",
    "        x = self.emb(x)#  埋め込み層\n",
    "        \n",
    "        y, h = self.rnn(x, h0)#  xと初期の隠れ状態h0でRNNモデルを実行\n",
    "        #print(y.size())\n",
    "        y = y[:, -1, :]#  時間経過の中で最後のステップを取得  #系列長、バッチサイズ、次元数\n",
    "        y = self.linear(y)#  全結合層 →　４次元へ\n",
    "        #y = self.softmax(y, dim=1)#  ４次元を確率へ\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "id": "a2b8cd15-2e3c-49be-b7f1-ef0719abebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        # データの読み出し\n",
    "        dataframe = []\n",
    "        with open(data_path) as f:\n",
    "            for line in f:\n",
    "                sent = line.split('\\t')\n",
    "                dataframe.append(sent)\n",
    "        self.dataframe = dataframe\n",
    "    # データのサイズ\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    # データとラベルの取得\n",
    "    def __getitem__(self, idx):\n",
    "        category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
    "        try:\n",
    "            label = category_dict[self.dataframe[idx][1].rstrip()]\n",
    "        except:\n",
    "            print(self.dataframe[idx][1].rstrip())\n",
    "        #print(label)\n",
    "        #print(self.dataframe[idx][0])\n",
    "        text_id = torch.tensor(text_2_id_list(self.dataframe[idx][0]), dtype=torch.int64)\n",
    "        # text_id, labelの順でリターン\n",
    "        return text_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "6be41ece-ba74-407e-b2f7-d1e2de30fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sec(batch):\n",
    "    batch_tensor = [item[0] for item in batch]\n",
    "    labels = torch.LongTensor([item[1] for item in batch])\n",
    "    pad_tensors = rnn.pad_sequence(batch_tensor,batch_first=True, padding_value=padding_idx)\n",
    "    return pad_tensors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "f8ae4fe2-d221-4d3c-abf3-3550b5a5efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "train_dataset = MyDataset('data/train.txt')\n",
    "valid_dataset = MyDataset('data/valid.txt')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=pad_sec)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=pad_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "id": "b4e829bd-2bf3-46ed-9c75-ad5449914560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  学習用の関数を定義\n",
    "\n",
    "\n",
    "\n",
    "def training(num_epochs):\n",
    "    model = My_rnn()\n",
    "    train_avg_loss = []\n",
    "    test_avg_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0  \n",
    "        train_pred_y_list = []\n",
    "        true_y_list = []\n",
    "        \n",
    "        for x_train_tensor, y_label in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(x_train_tensor)\n",
    "            loss = criterion(pred_y, y_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #予想したラベルを貯めておく\n",
    "            train_pred_y = torch.argmax(pred_y, dim=1)\n",
    "            train_pred_y_list.append(train_pred_y)\n",
    "            true_y_list.append(y_label)\n",
    "            \n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "            \n",
    "        \n",
    "        #  logの表示\n",
    "        print(f'epoch:{epoch}')\n",
    "        avg_loss = total_train_loss / len(train_pred_y_list)\n",
    "        train_avg_loss.append(avg_loss)\n",
    "        #print(pred_y)\n",
    "        #print(train_pred_y)\n",
    "        #print(train_pred_y_list[0])\n",
    "        #print(true_y_list[0])\n",
    "        train_y_pre = torch.cat(train_pred_y_list, dim = 0)\n",
    "        true_y_tensor = torch.cat(true_y_list, dim = 0)\n",
    "        \n",
    "        acc_score = accuracy_score(train_y_pre, true_y_tensor)\n",
    "        train_acc.append(acc_score)\n",
    "        print(f'acc:{acc_score}')\n",
    "        print(f'loss:{avg_loss}')\n",
    "        \n",
    "        \n",
    "    #print(\"train_loss\", train_avg_loss)\n",
    "    #print(\"train_acc\", train_acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "id": "90d94586-e2f9-4697-9a6d-a09977b364dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x, y in train_dataloader:\n",
    "    #print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "id": "83527695-4622-49d1-baa2-c12ac9fa1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_rnn(\n",
      "  (emb): Embedding(10139, 300, padding_idx=10138)\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(My_rnn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "id": "4f907542-12fd-4042-bd4b-9335ebdff0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▎                                                                                   | 1/10 [00:01<00:14,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "acc:0.39189442156495696\n",
      "loss:1.3647306711874276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████▌                                                                          | 2/10 [00:02<00:11,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n",
      "acc:0.4376637963309622\n",
      "loss:1.3129034231756336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████▉                                                                 | 3/10 [00:04<00:10,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2\n",
      "acc:0.44028453762635716\n",
      "loss:1.2710816977180053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████▏                                                       | 4/10 [00:05<00:08,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3\n",
      "acc:0.43934855859228755\n",
      "loss:1.2379926810754793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████▌                                              | 5/10 [00:07<00:07,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4\n",
      "acc:0.43962935230250844\n",
      "loss:1.2131600719746027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████▊                                     | 6/10 [00:08<00:05,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5\n",
      "acc:0.4409397229502059\n",
      "loss:1.1956773639839386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████                            | 7/10 [00:10<00:04,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6\n",
      "acc:0.44327967053538003\n",
      "loss:1.184014563805589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 8/10 [00:11<00:02,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7\n",
      "acc:0.4476787719955073\n",
      "loss:1.176461632563689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████▋         | 9/10 [00:13<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8\n",
      "acc:0.4487083489329839\n",
      "loss:1.1715686326829073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:14<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9\n",
      "acc:0.4517970797454137\n",
      "loss:1.1683023594250188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a38aa1-432f-46be-87dd-4327eee6cf4f",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49eae0-fb2b-46b6-9c72-7c8063693224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(num_epochs):\n",
    "    model = My_rnn()\n",
    "    train_avg_loss = []\n",
    "    test_avg_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    \n",
    "    # 初期設定\n",
    "    # GPUが使えるか確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(\"使用デバイス:\", device)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0  \n",
    "        train_pred_y_list = []\n",
    "        true_y_list = []\n",
    "        \n",
    "        for x_train_tensor, y_label in train_dataloader:\n",
    "            x_train_tensor = x_train_tensor.to(device)\n",
    "            y_label = y_label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(x_train_tensor)\n",
    "            loss = criterion(pred_y, y_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #予想したラベルを貯めておく\n",
    "            train_pred_y = torch.argmax(pred_y, dim=1)\n",
    "            train_pred_y_list.append(train_pred_y)\n",
    "            true_y_list.append(y_label)\n",
    "            \n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "            \n",
    "        \n",
    "        #  logの表示\n",
    "        print(f'epoch:{epoch}')\n",
    "        avg_loss = total_train_loss / len(train_pred_y_list)\n",
    "        train_avg_loss.append(avg_loss)\n",
    "        #print(pred_y)\n",
    "        #print(train_pred_y)\n",
    "        #print(train_pred_y_list[0])\n",
    "        #print(true_y_list[0])\n",
    "        train_y_pre = torch.cat(train_pred_y_list, dim = 0)\n",
    "        true_y_tensor = torch.cat(true_y_list, dim = 0)\n",
    "        \n",
    "        acc_score = accuracy_score(train_y_pre, true_y_tensor)\n",
    "        train_acc.append(acc_score)\n",
    "        print(f'acc:{acc_score}')\n",
    "        print(f'loss:{avg_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b37db-63cb-4885-82f0-2c261f98b573",
   "metadata": {},
   "source": [
    "batch_size=50\n",
    "num_epoch = 10\n",
    "NVIDIA GeForce GTX 1080 Ti\n",
    "使用デバイス: cuda:0\n",
    "  0%|                                                                                                     | 0/10 [00:00<?, ?it/s]\n",
    "epoch:0\n",
    "acc:0.22117184575065518\n",
    "loss:1.3987760009052597\n",
    " 10%|█████████▎                                                                                   | 1/10 [00:00<00:08,  1.10it/s]\n",
    "epoch:1\n",
    "acc:0.44009734181954324\n",
    "loss:1.334488627509536\n",
    " 20%|██████████████████▌                                                                          | 2/10 [00:01<00:07,  1.09it/s]\n",
    "epoch:2\n",
    "acc:0.44721078247847246\n",
    "loss:1.2850932400917339\n",
    " 30%|███████████████████████████▉                                                                 | 3/10 [00:02<00:06,  1.08it/s]\n",
    "epoch:3\n",
    "acc:0.4477723698989143\n",
    "loss:1.2465784989784812\n",
    " 40%|█████████████████████████████████████▏                                                       | 4/10 [00:03<00:05,  1.07it/s]\n",
    "epoch:4\n",
    "acc:0.44833395731935605\n",
    "loss:1.2178469194430057\n",
    " 50%|██████████████████████████████████████████████▌                                              | 5/10 [00:04<00:04,  1.07it/s]\n",
    "epoch:5\n",
    "acc:0.44926993635342566\n",
    "loss:1.1977538945518922\n",
    " 60%|███████████████████████████████████████████████████████▊                                     | 6/10 [00:05<00:03,  1.07it/s]\n",
    "epoch:6\n",
    "acc:0.4498315237738675\n",
    "loss:1.1844539280249693\n",
    " 70%|█████████████████████████████████████████████████████████████████                            | 7/10 [00:06<00:02,  1.07it/s]\n",
    "epoch:7\n",
    "acc:0.45273305877948333\n",
    "loss:1.1759229227204189\n",
    " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 8/10 [00:07<00:01,  1.07it/s]\n",
    "epoch:8\n",
    "acc:0.45479221265443653\n",
    "loss:1.1704850436371064\n",
    " 90%|███████████████████████████████████████████████████████████████████████████████████▋         | 9/10 [00:08<00:00,  1.06it/s]\n",
    "epoch:9\n",
    "acc:0.4559153874953201\n",
    "loss:1.1669585863563503\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.07it/s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b1ab4-76a3-48de-b58f-b7b322aca5ba",
   "metadata": {},
   "source": [
    "#  84. 単語ベクトルの導入\n",
    "\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)\n",
    "を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "f1aa7f22-4cab-44e6-b520-6679d7845b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n"
     ]
    }
   ],
   "source": [
    "trainee_n-miura/chapter07/data/GoogleNews-vectors-negative300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97729c4-913e-4ac6-9c92-affe75f44196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "\n",
    "\n",
    "# 学習済みモデルのロード\n",
    "key_model = gensim.models.KeyedVectors.load_word2vec_format(\"/Users/miuranaoki/nlp100/100knock-2023/trainee_n-miura/chapter07/data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2322a532-48e5-4bc0-a2ab-afeda6e44ae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_2_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#  隠れ状態ベクトルの次元数\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 8\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mword_2_id\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m#語彙サイズ＋１尚且つ０のやつもあるので＋１,\"-1\"の分で＋１\u001b[39;00m\n\u001b[1;32m      9\u001b[0m padding_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_2_id\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m emb_size \u001b[38;5;241m=\u001b[39m dw\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_2_id' is not defined"
     ]
    }
   ],
   "source": [
    "#ハイパラたち\n",
    "\n",
    "#  単語埋め込み次元数\n",
    "dw = 300\n",
    "#  隠れ状態ベクトルの次元数\n",
    "dh = 50\n",
    "\n",
    "input_size = len(word_2_id.values()) + 2 #語彙サイズ＋１尚且つ０のやつもあるので＋１,\"-1\"の分で＋１\n",
    "padding_idx = len(word_2_id.values()) +1\n",
    "emb_size = dw\n",
    "hidden_size = dh\n",
    "output_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcd863-1cfa-4d3a-9d67-0e1f1e6ca556",
   "metadata": {},
   "outputs": [],
   "source": [
    "syoki = np.zeros((input_size,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e84a2-0343-49b2-96bb-edf3b3f2d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "syoki.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "id": "f9ca1d32-1ad3-454c-9f34-a1a5d2bd88f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 1219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"apple\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "907e2adc-b59f-4cd0-afa9-d04680af8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for w, i in word_2_id.items():\n",
    "    try:\n",
    "        syoki[i] = key_model[w]\n",
    "        c += 1\n",
    "        #print(w)\n",
    "    except:\n",
    "        syoki[i] = np.random.normal(scale=1.0, size= (300, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75946b19-e9dd-4708-b783-529087c73c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8701/10139\n"
     ]
    }
   ],
   "source": [
    "print(f'{c}/{input_size}')\n",
    "weights = torch.from_numpy(syoki.astype((np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28bb2a48-0a87-488e-bb5a-b875214ee9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10139, 300])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b28f750-1431-42c2-8e87-0080ab261cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('work/weights.pickle', mode='wb') as f:\n",
    "    pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b23d83c7-9151-4800-b654-21354ebf7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('work/weights.pickle','rb') as fi:\n",
    "    weight = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f50f54f-ed8c-40a9-a106-70b594bd0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10139, 300])\n"
     ]
    }
   ],
   "source": [
    "print(weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "id": "0ecbf23d-152a-4906-b644-71f1c736d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_rnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #  埋め込みの定義\n",
    "        self.emb = nn.Embedding.from_pretrained(wei, padding_idx)\n",
    "        #  RNNの定義\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size,batch_first=True)#batch_size, seq_len, dim\n",
    "        #  出力層の定義\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        #  softmax層\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    #  予測関数の定義    \n",
    "    def forward(self, x, h0=None):\n",
    "        x = self.emb(x)#  埋め込み層\n",
    "        \n",
    "        y, h = self.rnn(x, h0)#  xと初期の隠れ状態h0でRNNモデルを実行\n",
    "        #print(y.size())\n",
    "        y = y[:, -1, :]#  時間経過の中で最後のステップを取得  #系列長、バッチサイズ、次元数\n",
    "        y = self.linear(y)#  全結合層 →　４次元へ\n",
    "        #y = self.softmax(y, dim=1)#  ４次元を確率へ\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "id": "1f8da6a1-f1db-4751-a990-9e5a64a53e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  学習用の関数を定義\n",
    "\n",
    "\n",
    "\n",
    "def training(num_epochs):\n",
    "    model = My_rnn()\n",
    "    train_avg_loss = []\n",
    "    test_avg_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0  \n",
    "        train_pred_y_list = []\n",
    "        true_y_list = []\n",
    "        \n",
    "        for x_train_tensor, y_label in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(x_train_tensor)\n",
    "            loss = criterion(pred_y, y_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #予想したラベルを貯めておく\n",
    "            train_pred_y = torch.argmax(pred_y, dim=1)\n",
    "            train_pred_y_list.append(train_pred_y)\n",
    "            true_y_list.append(y_label)\n",
    "            \n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "            \n",
    "        \n",
    "        #  logの表示\n",
    "        print(f'epoch:{epoch}')\n",
    "        avg_loss = total_train_loss / len(train_pred_y_list)\n",
    "        train_avg_loss.append(avg_loss)\n",
    "        #print(pred_y)\n",
    "        #print(train_pred_y)\n",
    "        #print(train_pred_y_list[0])\n",
    "        #print(true_y_list[0])\n",
    "        train_y_pre = torch.cat(train_pred_y_list, dim = 0)\n",
    "        true_y_tensor = torch.cat(true_y_list, dim = 0)\n",
    "        \n",
    "        acc_score = accuracy_score(train_y_pre, true_y_tensor)\n",
    "        train_acc.append(acc_score)\n",
    "        print(f'acc:{acc_score}')\n",
    "        print(f'loss:{avg_loss}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "id": "70194148-9e50-4fac-a456-b9192ae68032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10138"
      ]
     },
     "execution_count": 1273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_2_id.values()) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f861318-a37a-4fe9-a8b5-dd27ae6be7b4",
   "metadata": {},
   "source": [
    "batch_size=50\n",
    "num_epoch = 10\n",
    "NVIDIA GeForce GTX 1080 Ti\n",
    "使用デバイス: cuda:0\n",
    "  0%|                                                                                                                                                  | 0/10 [00:00<?, ?it/s]\n",
    "epoch:0\n",
    "acc:0.4190378135529764\n",
    "loss:1.3555285496132397\n",
    " 10%|█████████████▊                                                                                                                            | 1/10 [00:00<00:06,  1.42it/s]\n",
    "epoch:1\n",
    "acc:0.42250093597903404\n",
    "loss:1.3046491497030883\n",
    " 20%|███████████████████████████▌                                                                                                              | 2/10 [00:01<00:05,  1.43it/s]\n",
    "epoch:2\n",
    "acc:0.422688131785848\n",
    "loss:1.2655016122577347\n",
    " 30%|█████████████████████████████████████████▍                                                                                                | 3/10 [00:02<00:04,  1.43it/s]\n",
    "epoch:3\n",
    "acc:0.4232497192062898\n",
    "loss:1.2350091828364078\n",
    " 40%|███████████████████████████████████████████████████████▏                                                                                  | 4/10 [00:02<00:04,  1.44it/s]\n",
    "epoch:4\n",
    "acc:0.42381130662673155\n",
    "loss:1.2117288095928798\n",
    " 50%|█████████████████████████████████████████████████████████████████████                                                                     | 5/10 [00:03<00:03,  1.45it/s]\n",
    "epoch:5\n",
    "acc:0.4254960688880569\n",
    "loss:1.194669687859366\n",
    " 60%|██████████████████████████████████████████████████████████████████████████████████▊                                                       | 6/10 [00:04<00:02,  1.45it/s]\n",
    "epoch:6\n",
    "acc:0.42792961437663796\n",
    "loss:1.1827999715493105\n",
    " 70%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                         | 7/10 [00:04<00:02,  1.44it/s]\n",
    "epoch:7\n",
    "acc:0.4309247472856608\n",
    "loss:1.1749619407074474\n",
    " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 8/10 [00:05<00:01,  1.45it/s]\n",
    "epoch:8\n",
    "acc:0.43345189067764883\n",
    "loss:1.1700092726778761\n",
    " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 9/10 [00:06<00:00,  1.45it/s]\n",
    "epoch:9\n",
    "acc:0.43569824035941596\n",
    "loss:1.1669701131704813"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da652218-2861-45d0-b79b-7d6702d38c5a",
   "metadata": {},
   "source": [
    "#  85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ac3ca34-8d40-42e8-90a5-075150371253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_bi_rnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #  埋め込みの定義\n",
    "        self.emb = nn.Embedding.from_pretrained(weight, padding_idx)\n",
    "        #  RNNの定義\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size,batch_first=True,bidirectional=True,num_layers=3)#batch_size, seq_len, dim\n",
    "        #  出力層の定義\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        #  softmax層\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    #  予測関数の定義    \n",
    "    def forward(self, x, h0=None):\n",
    "        x = self.emb(x)#  埋め込み層\n",
    "        \n",
    "        y, h = self.rnn(x, h0)#  xと初期の隠れ状態h0でRNNモデルを実行\n",
    "        #print(y.size())\n",
    "        y = y[:, -1, :]#  時間経過の中で最後のステップを取得  #系列長、バッチサイズ、次元数\n",
    "        y = self.linear(y)#  全結合層 →　４次元へ\n",
    "        #y = self.softmax(y, dim=1)#  ４次元を確率へ\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6a5ab82-2b8d-4b2c-9166-992e10ac8db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_bi_rnn(\n",
      "  (emb): Embedding(10139, 300)\n",
      "  (rnn): RNN(300, 50, num_layers=3, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = My_bi_rnn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c4d34-67c8-4f3e-aa9c-e1fdc7696fa8",
   "metadata": {},
   "source": [
    "epoch:0\n",
    "acc:0.3379820292025459\n",
    "loss:1.3512836616730022\n",
    " 10%|█████████████▊                                                                                                                            | 1/10 [00:00<00:08,  1.12it/s]\n",
    "epoch:1\n",
    "acc:0.4280232122800449\n",
    "loss:1.2448863080728834\n",
    " 20%|███████████████████████████▌                                                                                                              | 2/10 [00:01<00:07,  1.12it/s]\n",
    "epoch:2\n",
    "acc:0.42933358292774243\n",
    "loss:1.19530474749681\n",
    " 30%|█████████████████████████████████████████▍                                                                                                | 3/10 [00:02<00:06,  1.13it/s]\n",
    "epoch:3\n",
    "acc:0.4324223137401722\n",
    "loss:1.1752584732581521\n",
    " 40%|███████████████████████████████████████████████████████▏                                                                                  | 4/10 [00:03<00:05,  1.13it/s]\n",
    "epoch:4\n",
    "acc:0.43775739423436916\n",
    "loss:1.167796624598102\n",
    " 50%|█████████████████████████████████████████████████████████████████████                                                                     | 5/10 [00:04<00:04,  1.15it/s]\n",
    "epoch:5\n",
    "acc:0.4404717334331711\n",
    "loss:1.1649580748281747\n",
    " 60%|██████████████████████████████████████████████████████████████████████████████████▊                                                       | 6/10 [00:05<00:03,  1.18it/s]\n",
    "epoch:6\n",
    "acc:0.44122051666042683\n",
    "loss:1.1637549971308663\n",
    " 70%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                         | 7/10 [00:06<00:02,  1.18it/s]\n",
    "epoch:7\n",
    "acc:0.4425308873081243\n",
    "loss:1.1631508413876328\n",
    " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 8/10 [00:06<00:01,  1.17it/s]\n",
    "epoch:8\n",
    "acc:0.4440284537626357\n",
    "loss:1.1627824426254378\n",
    " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 9/10 [00:07<00:00,  1.21it/s]\n",
    "epoch:9\n",
    "acc:0.4457132160239611\n",
    "loss:1.1625143320203941\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.17it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483159c-b2d9-478a-9edc-1a93f6340897",
   "metadata": {},
   "source": [
    "#  86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "eb2706be-3aaa-496f-86e3-46302bbc11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第１引数はインプットのチャネル（今回は1）を指定\n",
    "# 自然言語処理で畳み込む場合、異なる単語分散表現（word2vecとfasttextみたいな）などを使って、\n",
    "# 複数チャネルとみなす方法もあるようです。\n",
    "# 第２引数はアウトプットのチャネル数で、今回は同じフィルターを2枚畳み込みたいので、2を指定\n",
    "# カーネルサイズは高さ×幅を指定しており、幅は図で説明した通り、単語ベクトルの次元数5を指定\n",
    "\n",
    "\n",
    "\n",
    "dw = 300\n",
    "#  隠れ状態ベクトルの次元数\n",
    "dh = 50\n",
    "\n",
    "input_size = len(word_2_id.values()) + 2 #語彙サイズ＋１尚且つ０のやつもあるので＋１,\"-1\"の分で＋１\n",
    "padding_idx = len(word_2_id.values()) +1\n",
    "emb_size = dw\n",
    "hidden_size = dh\n",
    "output_size = 4\n",
    "\n",
    "out_channels = 100\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "369f82ae-696f-4990-86b2-f155483e47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('work/weights.pickle','rb') as fi:\n",
    "    weight = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2bee4999-10e4-4946-9028-1cb659b04b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #  埋め込みの定義\n",
    "        self.emb = nn.Embedding.from_pretrained(weight, padding_idx)\n",
    "        #  CNNの定義\n",
    "        self.conv = nn.Conv1d(dw, dh, kernel_size, stride, padding)# in_channels:dw, out_channels: dh\n",
    "        #  出力層の定義\n",
    "        self.linear = nn.Linear(dh, output_size)\n",
    "\n",
    "        \n",
    "    #  予測関数の定義    \n",
    "    def forward(self, x):\n",
    "        # x = [batch, 系列長]\n",
    "        x = self.emb(x)#  埋め込み層 torch.Size([6(系列長), 300])\n",
    "        x = x.unsqueeze(1)#  torch.Size([6, 300]) -> torch.Size([6, 300, 1])\n",
    "        #print(x.size())\n",
    "        x = x.view(x.shape[1], x.shape[2], x.shape[0])#  torch.Size([6, 300, 1]) -> torch.Size([1, 300, 6])\n",
    "        #print(x.size())\n",
    "        conv = self.conv(x)#  torch.Size([1, 300, 6]) -> torch.Size([1, 50, 6])\n",
    "        #print(conv.size())\n",
    "        x = F.relu(conv)\n",
    "        x = F.max_pool1d(x, x.size()[2])#  torch.Size([1, 50, 6]) -> torch.Size([1, 50, 1])\n",
    "        #print(x.size())\n",
    "        x = x.view(x.shape[0], x.shape[1])#  torch.Size([1, 50, 1]) -> torch.Size([1, 50])\n",
    "        #print(x.size())\n",
    "        #print(x[:, :, -1])\n",
    "        y = self.linear(x) # torch.Size([1, 50]) -> torch.Size([1, 4])\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5541d909-31a7-466b-8087-c31020da9f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 104, 2002,    1, 1533,    1,  301])\n",
      "torch.Size([6])\n",
      "tensor([[0.2544, 0.2197, 0.2197, 0.3062]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample = 'I want to go to Japan.'\n",
    "x = torch.tensor(text_2_id_list(sample))\n",
    "model = My_cnn()\n",
    "print(x)\n",
    "print(x.size())\n",
    "\n",
    "print(nn.Softmax(dim=-1)(model(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca090903-9ae8-4aff-8422-843a15144a9b",
   "metadata": {},
   "source": [
    "#  87. 確率的勾配降下法によるCNNの学習\n",
    "\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0af9e-c6c0-4c55-a9a2-b9a80dff3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #  埋め込みの定義\n",
    "        self.emb = nn.Embedding.from_pretrained(weights, padding_idx)\n",
    "        #  CNNの定義\n",
    "        self.conv = nn.Conv1d(dw, dh, kernel_size, stride, padding)# in_channels:dw, out_channels: dh\n",
    "        #  出力層の定義\n",
    "        self.linear = nn.Linear(dh, output_size)\n",
    "\n",
    "        \n",
    "    #  予測関数の定義    \n",
    "    def forward(self, x):\n",
    "        #print(x.size()) torch.Size([50, 15])\n",
    "        x = self.emb(x)#  埋め込み層 \n",
    "        #print(x.size()) torch.Size([50, 15, 300])\n",
    "        x = x.view(x.shape[0], x.shape[2], x.shape[1])\n",
    "        #print(x.size()) #torch.Size([50, 300, 15])\n",
    "        conv = self.conv(x)\n",
    "        #print(conv.size()) # torch.Size([50, 50, 15])\n",
    "        x = F.relu(conv)\n",
    "        x = F.max_pool1d(x, x.size()[2])\n",
    "        #print(x.size()) # torch.Size([50, 50, 1])\n",
    "        x = x.view(x.shape[0], x.shape[1])\n",
    "        #print(x.size()) #  torch.Size([50, 50])\n",
    "        #print(x[:, :, -1])\n",
    "        y = self.linear(x) \n",
    "        #print(y.size()) # torch.Size([50, 4])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2711ded-d0c8-4212-a7ab-a17843139b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_valid(num_epochs):\n",
    "    model = My_cnn()\n",
    "    train_avg_loss = []\n",
    "    valid_avg_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "\n",
    "    # 初期設定\n",
    "    # GPUが使えるか確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(\"使用デバイス:\", device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0 \n",
    "        valid_loss = 0\n",
    "        correct_valid = 0 \n",
    "        train_pred_y_list = []\n",
    "        valid_pred_y_list = []\n",
    "        true_y_list = []\n",
    "        valid_true_y_list = []\n",
    "        \n",
    "        for x_train_tensor, y_label in train_dataloader:\n",
    "            x_train_tensor = x_train_tensor.to(device)\n",
    "            y_label = y_label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(x_train_tensor)\n",
    "            #print(pred_y.size())\n",
    "            #print(y_label.size())\n",
    "            loss = criterion(pred_y, y_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #予想したラベルを貯めておく\n",
    "            train_pred_y = torch.argmax(pred_y, dim=1)\n",
    "            train_pred_y_list.append(train_pred_y)\n",
    "            true_y_list.append(y_label)\n",
    "            \n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            for x_valid_tensor, y_valid_label in valid_dataloader:\n",
    "                x_valid_tensor = x_valid_tensor.to(device)\n",
    "                y_valid_label = y_valid_label.to(device)\n",
    "                valid_pred_y = model(x_valid_tensor)\n",
    "                loss = criterion(valid_pred_y, y_valid_label)\n",
    "                total_valid_loss += loss.item()\n",
    "                #予想したラベルを貯めておく\n",
    "                valid_pred_y = torch.argmax(valid_pred_y, dim=1)\n",
    "                valid_pred_y_list.append(valid_pred_y)\n",
    "                valid_true_y_list.append(y_valid_label)\n",
    "\n",
    "        #  logの表示\n",
    "        print(f'epoch:{epoch}')\n",
    "        tra_avg_loss = total_train_loss / len(train_pred_y_list)\n",
    "        val_avg_loss = total_valid_loss / len(valid_pred_y_list)\n",
    "        train_avg_loss.append(tra_avg_loss)\n",
    "        valid_avg_loss.append(val_avg_loss)\n",
    "\n",
    "        train_y_pre = torch.cat(train_pred_y_list, dim = 0)\n",
    "        valid_y_pre = torch.cat(valid_pred_y_list, dim = 0)\n",
    "\n",
    "        true_y_tensor = torch.cat(true_y_list, dim = 0)\n",
    "        valid_true_y_tensor = torch.cat(valid_true_y_list, dim = 0)\n",
    "\n",
    "        train_y_pre = train_y_pre.cpu()\n",
    "        valid_y_pre = valid_y_pre.cpu()\n",
    "\n",
    "        true_y_tensor = true_y_tensor.cpu()\n",
    "        valid_true_y_tensor = valid_true_y_tensor.cpu()\n",
    "\n",
    "        acc_score = accuracy_score(train_y_pre, true_y_tensor)\n",
    "        valid_acc_score = accuracy_score(valid_y_pre, valid_true_y_tensor)\n",
    "\n",
    "        train_acc.append(acc_score)\n",
    "        valid_acc.append(valid_acc_score)\n",
    "\n",
    "        print(f'train_acc:{acc_score}')\n",
    "        print(f'valid_acc:{valid_acc_score}')\n",
    "\n",
    "        print(f'train_avg_loss:{statistics.mean(train_avg_loss)}')\n",
    "        print(f'valid_avg_loss:{statistics.mean(valid_avg_loss)}')\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "num_epoch = 5\n",
    "batch_size = 10\n",
    "\n",
    "#  main\n",
    "if __name__ == '__main__':\n",
    "    print(f'batch_size = {batch_size}')\n",
    "    print(f'num_epoch = {num_epoch}')\n",
    "\n",
    "    training_and_valid(num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a19c189-930c-4df7-b5d8-518d99ee018c",
   "metadata": {},
   "source": [
    " % python q87.py\n",
    "batch_size = 10\n",
    "num_epoch = 5\n",
    "NVIDIA GeForce GTX 1080 Ti\n",
    "使用デバイス: cuda:0\n",
    "  0%|                                                                                       | 0/5 [00:00<?, ?it/s]\n",
    "epoch:0\n",
    "train_acc:0.37411081991763384\n",
    "valid_acc:0.4865269461077844\n",
    "train_avg_loss:1.3224773526303226\n",
    "valid_avg_loss:1.189154571561671\n",
    " 20%|███████████████▊                                                               | 1/5 [00:02<00:10,  2.69s/it]\n",
    "epoch:1\n",
    "train_acc:0.484462748034444\n",
    "valid_acc:0.49850299401197606\n",
    "train_avg_loss:1.2469308870322688\n",
    "valid_avg_loss:1.1653455655966232\n",
    " 40%|███████████████████████████████▌                                               | 2/5 [00:05<00:07,  2.47s/it]\n",
    "epoch:2\n",
    "train_acc:0.4928865593410708\n",
    "valid_acc:0.5217065868263473\n",
    "train_avg_loss:1.21415727566736\n",
    "valid_avg_loss:1.1525217235384888\n",
    " 60%|███████████████████████████████████████████████▍                               | 3/5 [00:07<00:04,  2.37s/it]\n",
    "epoch:3\n",
    "train_acc:0.510108573567952\n",
    "valid_acc:0.5284431137724551\n",
    "train_avg_loss:1.1947822521037315\n",
    "valid_avg_loss:1.1436533074992807\n",
    " 80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:09<00:02,  2.34s/it]\n",
    "epoch:4\n",
    "train_acc:0.5203107450393111\n",
    "valid_acc:0.5276946107784432\n",
    "train_avg_loss:1.1810466110316027\n",
    "valid_avg_loss:1.1365581802467801\n",
    "100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:11<00:00,  2.37s/it]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57ee52-4584-4a47-998f-de923808a008",
   "metadata": {},
   "source": [
    "#  88. パラメータチューニング\n",
    "\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9424c-369c-49f1-8790-1c36c25be623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_valid(num_epochs):\n",
    "    model = My_cnn()\n",
    "    train_avg_loss = []\n",
    "    valid_avg_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    model.train()\n",
    "\n",
    "    # 初期設定\n",
    "    # GPUが使えるか確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(\"使用デバイス:\", device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0 \n",
    "        valid_loss = 0\n",
    "        correct_valid = 0 \n",
    "        train_pred_y_list = []\n",
    "        valid_pred_y_list = []\n",
    "        true_y_list = []\n",
    "        valid_true_y_list = []\n",
    "        \n",
    "        for x_train_tensor, y_label in train_dataloader:\n",
    "            x_train_tensor = x_train_tensor.to(device)\n",
    "            y_label = y_label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(x_train_tensor)\n",
    "            #print(pred_y.size())\n",
    "            #print(y_label.size())\n",
    "            loss = criterion(pred_y, y_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #予想したラベルを貯めておく\n",
    "            train_pred_y = torch.argmax(pred_y, dim=1)\n",
    "            train_pred_y_list.append(train_pred_y)\n",
    "            true_y_list.append(y_label)\n",
    "            \n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            for x_valid_tensor, y_valid_label in valid_dataloader:\n",
    "                x_valid_tensor = x_valid_tensor.to(device)\n",
    "                y_valid_label = y_valid_label.to(device)\n",
    "                valid_pred_y = model(x_valid_tensor)\n",
    "                loss = criterion(valid_pred_y, y_valid_label)\n",
    "                total_valid_loss += loss.item()\n",
    "                #予想したラベルを貯めておく\n",
    "                valid_pred_y = torch.argmax(valid_pred_y, dim=1)\n",
    "                valid_pred_y_list.append(valid_pred_y)\n",
    "                valid_true_y_list.append(y_valid_label)\n",
    "\n",
    "        #  logの表示\n",
    "        print(f'epoch:{epoch}')\n",
    "        tra_avg_loss = total_train_loss / len(train_pred_y_list)\n",
    "        val_avg_loss = total_valid_loss / len(valid_pred_y_list)\n",
    "        train_avg_loss.append(tra_avg_loss)\n",
    "        valid_avg_loss.append(val_avg_loss)\n",
    "\n",
    "        train_y_pre = torch.cat(train_pred_y_list, dim = 0)\n",
    "        valid_y_pre = torch.cat(valid_pred_y_list, dim = 0)\n",
    "\n",
    "        true_y_tensor = torch.cat(true_y_list, dim = 0)\n",
    "        valid_true_y_tensor = torch.cat(valid_true_y_list, dim = 0)\n",
    "\n",
    "        train_y_pre = train_y_pre.cpu()\n",
    "        valid_y_pre = valid_y_pre.cpu()\n",
    "\n",
    "        true_y_tensor = true_y_tensor.cpu()\n",
    "        valid_true_y_tensor = valid_true_y_tensor.cpu()\n",
    "\n",
    "        acc_score = accuracy_score(train_y_pre, true_y_tensor)\n",
    "        valid_acc_score = accuracy_score(valid_y_pre, valid_true_y_tensor)\n",
    "\n",
    "        train_acc.append(acc_score)\n",
    "        valid_acc.append(valid_acc_score)\n",
    "\n",
    "        print(f'train_acc:{acc_score}')\n",
    "        print(f'valid_acc:{valid_acc_score}')\n",
    "\n",
    "        print(f'train_avg_loss:{statistics.mean(train_avg_loss)}')\n",
    "        print(f'valid_avg_loss:{statistics.mean(valid_avg_loss)}')\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "num_epoch = 5\n",
    "batch_size = 10\n",
    "\n",
    "#  main\n",
    "if __name__ == '__main__':\n",
    "    print(f'batch_size = {batch_size}')\n",
    "    print(f'num_epoch = {num_epoch}')\n",
    "\n",
    "    training_and_valid(num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843b7da-10ad-48c1-aa77-defc9cbeb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "% python q88.py\n",
    "batch_size = 10\n",
    "num_epoch = 5\n",
    "NVIDIA GeForce GTX 1080 Ti\n",
    "使用デバイス: cuda:0\n",
    "  0%|                                                                                       | 0/5 [00:00<?, ?it/s]\n",
    "epoch:0\n",
    "train_acc:0.5964994384125796\n",
    "valid_acc:0.6392215568862275\n",
    "train_avg_loss:1.033733327551575\n",
    "valid_avg_loss:0.9463398832438598\n",
    " 20%|███████████████▊                                                               | 1/5 [00:03<00:12,  3.23s/it]\n",
    "epoch:1\n",
    "train_acc:0.7048858105578435\n",
    "valid_acc:0.6526946107784432\n",
    "train_avg_loss:0.9151883647751875\n",
    "valid_avg_loss:0.933617778900844\n",
    " 40%|███████████████████████████████▌                                               | 2/5 [00:05<00:08,  2.83s/it]\n",
    "epoch:2\n",
    "train_acc:0.791463871209285\n",
    "valid_acc:0.6422155688622755\n",
    "train_avg_loss:0.8048011283679175\n",
    "valid_avg_loss:0.9401777177604277\n",
    " 60%|███████████████████████████████████████████████▍                               | 3/5 [00:08<00:05,  2.68s/it]\n",
    "epoch:3\n",
    "train_acc:0.863253463122426\n",
    "valid_acc:0.6354790419161677\n",
    "train_avg_loss:0.7067855492598396\n",
    "valid_avg_loss:0.9559657252880175\n",
    " 80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:10<00:02,  2.60s/it]\n",
    "epoch:4\n",
    "train_acc:0.9126731561213028\n",
    "valid_acc:0.6317365269461078\n",
    "train_avg_loss:0.6240185820416551\n",
    "valid_avg_loss:0.9802028477414331\n",
    "100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.67s/it]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
