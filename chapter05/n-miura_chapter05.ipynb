{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4795f3ec-747a-414f-aea3-2abdcda37f5a",
   "metadata": {},
   "source": [
    "データ整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766b0dfd-b696-46d0-89a7-eb4605b5c8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-12 13:55:40--  https://nlp100.github.io/data/ai.ja.zip\n",
      "nlp100.github.io (nlp100.github.io) をDNSに問いあわせています... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n",
      "nlp100.github.io (nlp100.github.io)|185.199.108.153|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 17516 (17K) [application/zip]\n",
      "`ai.ja.zip' に保存中\n",
      "\n",
      "ai.ja.zip           100%[===================>]  17.11K  --.-KB/s 時間 0.001s     \n",
      "\n",
      "2023-05-12 13:55:40 (13.3 MB/s) - `ai.ja.zip' へ保存完了 [17516/17516]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp100.github.io/data/ai.ja.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53805084-fff3-4270-a8d5-3deb3340dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai.ja.txt', 'readme.ai.ja.md']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "zip_f =  zipfile.ZipFile('data/ai.ja.zip')\n",
    "zip_list = zip_f.namelist()\n",
    "print(zip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2faf8ae2-eb81-4c4f-9dce-a4ed3d4d860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/ai.ja.zip') as myzip:\n",
    "    myzip.extract('ai.ja.txt', path = './work')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f9180-d564-44bc-a55e-a2acaae26960",
   "metadata": {},
   "source": [
    "ファイルに入っている文章を一文1行にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25f89c8b-5cee-4576-87d3-b97b694f5e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (23.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (67.6.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-67.8.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (0.38.4)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wheel, setuptools\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.38.4\n",
      "    Uninstalling wheel-0.38.4:\n",
      "      Successfully uninstalled wheel-0.38.4\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 67.6.0\n",
      "    Uninstalling setuptools-67.6.0:\n",
      "      Successfully uninstalled setuptools-67.6.0\n",
      "Successfully installed setuptools-67.8.0 wheel-0.40.0\n",
      "Requirement already satisfied: spacy in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (3.5.0)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp38-cp38-macosx_10_9_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy)\n",
      "  Downloading thinc-8.1.10-cp38-cp38-macosx_10_9_x86_64.whl (850 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.2/850.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (1.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: thinc, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.7\n",
      "    Uninstalling thinc-8.1.7:\n",
      "      Successfully uninstalled thinc-8.1.7\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.5.0\n",
      "    Uninstalling spacy-3.5.0:\n",
      "      Successfully uninstalled spacy-3.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ginza 5.1.2 requires spacy<3.5.0,>=3.2.0, but you have spacy 3.5.3 which is incompatible.\n",
      "ja-ginza 5.1.2 requires spacy<3.5.0,>=3.2.0, but you have spacy 3.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed spacy-3.5.3 thinc-8.1.10\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/miuranaoki/.pyenv/versions/3.8.11/envs/100knock/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe4fef5-76a4-483a-a916-8c2c364b7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "with open('work/ai.ja.txt') as fi,open('work/ai.ja.txt.ginza','w') as fo:\n",
    "    for line in fi:\n",
    "        doc = nlp(line.rstrip())\n",
    "        for sent in doc.sents:#doc.sents で文章のジェネレータ\n",
    "            fo.write(str(sent) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60858229-2b04-47ee-9258-a8272bce1ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\n",
      "人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。\n",
      "「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。\n",
      "『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている。\n",
      "人間の知的能力をコンピュータ上で実現する、様々な技術・ソフトウェア・コンピュータシステム。\n",
      "応用例は自然言語処理（機械翻訳・かな漢字変換・構文解析等）、専門家の推論・判断を模倣するエキスパートシステム、画像データを解析して特定のパターンを検出・抽出したりする画像認識等がある。\n",
      "1956年にダートマス会議でジョン・マッカーシーにより命名された。\n",
      "現在では、記号処理を用いた知能の記述を主体とする情報処理や研究でのアプローチという意味あいでも使われている。\n",
      "家庭用電気機械器具の制御システムやゲームソフトの思考ルーチンもこう呼ばれることもある。\n",
      "プログラミング言語 による「」というカウンセラーを模倣したプログラム（人工無脳）がしばしば引き合いに出されるが、計算機に人間の専門家の役割をさせようという「エキスパートシステム」と呼ばれる研究・情報処理システムの実現は、人間が暗黙に持つ常識の記述が問題となり、実用への利用が困難視されている。\n"
     ]
    }
   ],
   "source": [
    "!head work/ai.ja.txt.ginza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd53f8c8-1597-4519-8b36-ceeb534f5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "with open('work/ai.ja.txt.ginza') as fi,open('work/ai.ja.txt.parsed','w') as fo:\n",
    "    c = CaboCha.Parser()#カボチャのparserクラスにアクセス\n",
    "    for sentence in fi:\n",
    "        tree = c.parse(sentence)#Parserのメソッドparse()を使って構文木を生成\n",
    "        result = tree.toString(CaboCha.FORMAT_LATTICE)#構文木Treeのメソッド,toString()に情報の出力方法を指定して呼び出し\n",
    "        fo.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48acf9dc-403d-4446-9cc0-3ec543e5b632",
   "metadata": {},
   "source": [
    "#40 係り受け解析結果の読み込み（形態素）\n",
    "\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "768917a0-48f6-421a-af00-e2e0c1292f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deed4d07-8bc8-4ee1-9444-01a99fb102d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ai_sentences_list = []\n",
    "ai_sentence_list = []\n",
    "import re\n",
    "\n",
    "with open('work/ai.ja.txt.parsed') as fi:\n",
    "    for line in fi:#一文1行\n",
    "        line_info = re.split(\"['\\t', ]\",line.rstrip())\n",
    "        #print(line_info)\n",
    "        if line_info[0] == 'EOS' and len(ai_sentence_list):\n",
    "            all_ai_sentences_list.append(ai_sentence_list)\n",
    "            ai_sentence_list = []\n",
    "        elif line_info[0] == '*':\n",
    "            continue\n",
    "        else:\n",
    "            surface = line_info[0]\n",
    "            base = line_info[7]\n",
    "            pos = line_info[1]\n",
    "            pos1 = line_info[2]\n",
    "            m = Morph(surface, base, pos, pos1)\n",
    "            ai_sentence_list.append(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2a1e2-c67c-4fb1-beae-daa2cc353f52",
   "metadata": {},
   "source": [
    "#41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84730547-a020-4e4c-bbb1-61714b128e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1, chunk):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        self.chunk = chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1082bf-c0fc-47df-a84f-3c1f92cc672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, idx, dst, sentence):\n",
    "        self.morphs = []\n",
    "        self.dst = dst\n",
    "        self.srcs = []\n",
    "        \n",
    "        self.sentence = sentence\n",
    "        self.idx = idx\n",
    "    \n",
    "    def surface(self):#　　文節内の形態素の集合を文字列にして出力するメソッド\n",
    "        chk_word = \"\"\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos != '記号':#記号を排除\n",
    "                chk_word += morph.surface\n",
    "        return chk_word\n",
    "    \n",
    "    def verb_base(self):#  最左の動詞を返す\n",
    "        v = []\n",
    "        if self.has_pos(\"動詞\"):\n",
    "            for morph in self.morphs:\n",
    "                 if morph.pos == \"動詞\":\n",
    "                        v.append(morph.base)\n",
    "                        return next(iter(v))\n",
    "                        \n",
    "            \n",
    "    \n",
    "    def has_dst(self):#　　係り先あるか\n",
    "        if self.dst != -1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def to_dst_chunk(self):#　　係り先のchunkの表層形を表示\n",
    "        return self.sentence[self.dst].surface()\n",
    "    \n",
    "    def has_pos(self,pos):#　　chunkが指定した品詞をもっているか確認できる\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def has_pos1(self,pos1):#　　chunkが指定した品詞をもっているか確認できる\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos1 == pos1:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def has_word(self, word, pos):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos and morph.surface == word:\n",
    "                return True\n",
    "        return False\n",
    "   \n",
    "    \n",
    "    \n",
    "    def has_srcs(self):#  係りもととなっているか\n",
    "        if self.srcs:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "    def of_particle(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"助詞\":\n",
    "                return morph.base\n",
    "    \n",
    "    def get_particle(self):#  右側から助詞を取得してくる\n",
    "        particle_list = []\n",
    "        if self.morphs[-1].pos  == \"助詞\":\n",
    "            for m in reversed(self.morphs):\n",
    "                if m.pos == \"助詞\":\n",
    "                    particle_list.append(m.surface)\n",
    "                else:\n",
    "                    return ''.join(reversed(particle_list))\n",
    "                    \n",
    "               \n",
    "                    \n",
    "    \n",
    "    #　　表示用\n",
    "    def __str__(self):\n",
    "        morphs_str = ''.join(m.surface for m in self.morphs)\n",
    "        return f'文節番号： {self.idx},\\t係り先(dst)： {self.dst},\\t係り元(srcs): {self.srcs},\\tmorphs: {morphs_str}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fdf1af9-165e-42de-9ad6-573c2271a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "all_sentences_list = []\n",
    "sentence_list = []\n",
    "\n",
    "\n",
    "with open('work/ai.ja.txt.parsed') as f:\n",
    "    \n",
    "    for line in f:#一文1行\n",
    "        line_info = re.split(\"[\\t, ]\",line.rstrip())\n",
    "        \n",
    "        if line_info[0] == \"*\" :# \"*\"（文節情報の行）来た時\n",
    "            idx = int(line_info[1])\n",
    "            dst = int(line_info[2][:-1])#Dを避ける\n",
    "            chunk = Chunk(idx, dst, sentence_list)\n",
    "            sentence_list.append(chunk)\n",
    "            \n",
    "        elif line_info[0] == \"EOS\":#文の終端記号\n",
    "            for chunk in sentence_list:\n",
    "                sentence_list[chunk.dst].srcs.append(chunk.idx)#srcsに係り元番号の追加\n",
    "            \n",
    "            all_sentences_list.append(sentence_list)\n",
    "            sentence_list = []\n",
    "            \n",
    "        else:#形態素がきた場合\n",
    "            surface = line_info[0]\n",
    "            base = line_info[7]\n",
    "            pos = line_info[1]\n",
    "            pos1 = line_info[2]\n",
    "            m = Morph(surface, base, pos, pos1, chunk)\n",
    "            chunk.morphs.append(m)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c391ab9-e4a8-403d-905c-cc805af0a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文節番号： 0,\t係り先(dst)： 17,\t係り元(srcs): [],\tmorphs: 人工知能\n",
      "文節番号： 1,\t係り先(dst)： 17,\t係り元(srcs): [],\tmorphs: （じんこうちのう、、\n",
      "文節番号： 2,\t係り先(dst)： 3,\t係り元(srcs): [],\tmorphs: AI\n",
      "文節番号： 3,\t係り先(dst)： 17,\t係り元(srcs): [2],\tmorphs: 〈エーアイ〉）とは、\n",
      "文節番号： 4,\t係り先(dst)： 5,\t係り元(srcs): [],\tmorphs: 「『計算\n",
      "文節番号： 5,\t係り先(dst)： 9,\t係り元(srcs): [4],\tmorphs: （）』という\n",
      "文節番号： 6,\t係り先(dst)： 9,\t係り元(srcs): [],\tmorphs: 概念と\n",
      "文節番号： 7,\t係り先(dst)： 8,\t係り元(srcs): [],\tmorphs: 『コンピュータ\n",
      "文節番号： 8,\t係り先(dst)： 9,\t係り元(srcs): [7],\tmorphs: （）』という\n",
      "文節番号： 9,\t係り先(dst)： 10,\t係り元(srcs): [5, 6, 8],\tmorphs: 道具を\n",
      "文節番号： 10,\t係り先(dst)： 12,\t係り元(srcs): [9],\tmorphs: 用いて\n",
      "文節番号： 11,\t係り先(dst)： 12,\t係り元(srcs): [],\tmorphs: 『知能』を\n",
      "文節番号： 12,\t係り先(dst)： 13,\t係り元(srcs): [10, 11],\tmorphs: 研究する\n",
      "文節番号： 13,\t係り先(dst)： 14,\t係り元(srcs): [12],\tmorphs: 計算機科学\n",
      "文節番号： 14,\t係り先(dst)： 15,\t係り元(srcs): [13],\tmorphs: （）の\n",
      "文節番号： 15,\t係り先(dst)： 16,\t係り元(srcs): [14],\tmorphs: 一分野」を\n",
      "文節番号： 16,\t係り先(dst)： 17,\t係り元(srcs): [15],\tmorphs: 指す\n",
      "文節番号： 17,\t係り先(dst)： -1,\t係り元(srcs): [0, 1, 3, 16, 17],\tmorphs: 語。\n"
     ]
    }
   ],
   "source": [
    "for chunk in all_sentences_list[1]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67bc8f-46ec-4406-80fc-cf63b3fa46ca",
   "metadata": {},
   "source": [
    "#42. 係り元と係り先の文節の表示\n",
    "\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2610908b-a396-4d49-a39b-1862d3f4431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "係り元：人工知能\t係り先：語\n",
      "係り元：じんこうちのう\t係り先：語\n",
      "係り元：AI\t係り先：エーアイとは\n",
      "係り元：エーアイとは\t係り先：語\n",
      "係り元：計算\t係り先：という\n",
      "係り元：という\t係り先：道具を\n",
      "係り元：概念と\t係り先：道具を\n",
      "係り元：コンピュータ\t係り先：という\n",
      "係り元：という\t係り先：道具を\n",
      "係り元：道具を\t係り先：用いて\n",
      "係り元：用いて\t係り先：研究する\n",
      "係り元：知能を\t係り先：研究する\n",
      "係り元：研究する\t係り先：計算機科学\n",
      "係り元：計算機科学\t係り先：の\n",
      "係り元：の\t係り先：一分野を\n",
      "係り元：一分野を\t係り先：指す\n",
      "係り元：指す\t係り先：語\n"
     ]
    }
   ],
   "source": [
    "for chunk in all_sentences_list[1]:\n",
    "    if chunk.has_dst():\n",
    "        print(f'係り元：{chunk.surface()}\\t係り先：{chunk.to_dst_chunk()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95750e41-a379-4e82-93f5-acaa45a1491e",
   "metadata": {},
   "source": [
    "#43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b96759-cc50-4e01-a840-27c2a31121a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "係り元（名詞）：道具を\t係り先(動詞)：用いて\n",
      "係り元（名詞）：知能を\t係り先(動詞)：研究する\n",
      "係り元（名詞）：一分野を\t係り先(動詞)：指す\n"
     ]
    }
   ],
   "source": [
    "for chunk in all_sentences_list[1]:\n",
    "    if chunk.has_pos('名詞')and chunk.has_dst:\n",
    "        if chunk.sentence[chunk.dst].has_pos('動詞'):\n",
    "            print(f'係り元（名詞）：{chunk.surface()}\\t係り先(動詞)：{chunk.to_dst_chunk()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43721d5b-9335-482c-b497-a5e9a9a98c72",
   "metadata": {},
   "source": [
    "#44. 係り受け木の可視化\n",
    "\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ebf5c4-9d7a-4bbd-b0c2-96460e82c4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.1.0 (20230121.1956)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"553pt\" height=\"692pt\"\n",
       " viewBox=\"0.00 0.00 553.44 692.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 688)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-688 549.44,-688 549.44,4 -4,4\"/>\n",
       "<!-- 人工知能 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>人工知能</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"48.1\" cy=\"-90\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"48.1\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">人工知能</text>\n",
       "</g>\n",
       "<!-- 語 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>語</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"268.1\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.1\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">語</text>\n",
       "</g>\n",
       "<!-- 人工知能&#45;&gt;語 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>人工知能&#45;&gt;語</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M83.93,-77.6C125.11,-64.5 192.24,-43.13 233.08,-30.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.94,-33.54 242.41,-27.17 231.82,-26.87 233.94,-33.54\"/>\n",
       "</g>\n",
       "<!-- じんこうちのう -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>じんこうちのう</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"189.1\" cy=\"-90\" rx=\"75.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.1\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">じんこうちのう</text>\n",
       "</g>\n",
       "<!-- じんこうちのう&#45;&gt;語 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>じんこうちのう&#45;&gt;語</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M207.82,-72.41C218.65,-62.81 232.4,-50.63 244.04,-40.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"246.14,-43.13 251.3,-33.88 241.49,-37.89 246.14,-43.13\"/>\n",
       "</g>\n",
       "<!-- AI -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>AI</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"347.1\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.1\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">AI</text>\n",
       "</g>\n",
       "<!-- エーアイとは -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>エーアイとは</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"347.1\" cy=\"-90\" rx=\"64.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.1\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">エーアイとは</text>\n",
       "</g>\n",
       "<!-- AI&#45;&gt;エーアイとは -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>AI&#45;&gt;エーアイとは</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M347.1,-143.7C347.1,-136.41 347.1,-127.73 347.1,-119.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.6,-119.62 347.1,-109.62 343.6,-119.62 350.6,-119.62\"/>\n",
       "</g>\n",
       "<!-- エーアイとは&#45;&gt;語 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>エーアイとは&#45;&gt;語</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M328.37,-72.41C317.54,-62.81 303.79,-50.63 292.16,-40.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.7,-37.89 284.89,-33.88 290.05,-43.13 294.7,-37.89\"/>\n",
       "</g>\n",
       "<!-- 計算 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>計算</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"356.1\" cy=\"-666\" rx=\"29.5\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"356.1\" y=\"-662.3\" font-family=\"Times,serif\" font-size=\"14.00\">計算</text>\n",
       "</g>\n",
       "<!-- という -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>という</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"412.1\" cy=\"-594\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"412.1\" y=\"-590.3\" font-family=\"Times,serif\" font-size=\"14.00\">という</text>\n",
       "</g>\n",
       "<!-- 計算&#45;&gt;という -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>計算&#45;&gt;という</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M368.52,-649.46C375.46,-640.8 384.26,-629.79 392.14,-619.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"394.85,-622.15 398.37,-612.16 389.39,-617.78 394.85,-622.15\"/>\n",
       "</g>\n",
       "<!-- 道具を -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>道具を</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"412.1\" cy=\"-522\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"412.1\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\">道具を</text>\n",
       "</g>\n",
       "<!-- という&#45;&gt;道具を -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>という&#45;&gt;道具を</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M406.22,-576.05C405.44,-568.68 405.18,-559.84 405.45,-551.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"408.93,-551.99 406.08,-541.79 401.94,-551.54 408.93,-551.99\"/>\n",
       "</g>\n",
       "<!-- という&#45;&gt;道具を -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>という&#45;&gt;道具を</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M417.97,-576.05C418.76,-568.68 419.01,-559.84 418.74,-551.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"422.25,-551.54 418.11,-541.79 415.27,-551.99 422.25,-551.54\"/>\n",
       "</g>\n",
       "<!-- 用いて -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>用いて</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"412.1\" cy=\"-450\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"412.1\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">用いて</text>\n",
       "</g>\n",
       "<!-- 道具を&#45;&gt;用いて -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>道具を&#45;&gt;用いて</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M412.1,-503.7C412.1,-496.41 412.1,-487.73 412.1,-479.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"415.6,-479.62 412.1,-469.62 408.6,-479.62 415.6,-479.62\"/>\n",
       "</g>\n",
       "<!-- 概念と -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>概念と</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"507.1\" cy=\"-594\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"507.1\" y=\"-590.3\" font-family=\"Times,serif\" font-size=\"14.00\">概念と</text>\n",
       "</g>\n",
       "<!-- 概念と&#45;&gt;道具を -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>概念と&#45;&gt;道具を</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M487.41,-578.5C474,-568.61 456.02,-555.36 440.99,-544.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"443.43,-541.74 433.3,-538.62 439.27,-547.37 443.43,-541.74\"/>\n",
       "</g>\n",
       "<!-- コンピュータ -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>コンピュータ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"469.1\" cy=\"-666\" rx=\"65.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"469.1\" y=\"-662.3\" font-family=\"Times,serif\" font-size=\"14.00\">コンピュータ</text>\n",
       "</g>\n",
       "<!-- コンピュータ&#45;&gt;という -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>コンピュータ&#45;&gt;という</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M455.3,-648.05C448.48,-639.68 440.11,-629.4 432.56,-620.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"435.36,-618.03 426.33,-612.48 429.93,-622.45 435.36,-618.03\"/>\n",
       "</g>\n",
       "<!-- 研究する -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>研究する</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459.1\" cy=\"-378\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459.1\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">研究する</text>\n",
       "</g>\n",
       "<!-- 用いて&#45;&gt;研究する -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>用いて&#45;&gt;研究する</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M423.23,-432.41C428.66,-424.34 435.31,-414.43 441.4,-405.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"444.2,-407.47 446.87,-397.21 438.39,-403.56 444.2,-407.47\"/>\n",
       "</g>\n",
       "<!-- 計算機科学 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>計算機科学</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459.1\" cy=\"-306\" rx=\"57.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459.1\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">計算機科学</text>\n",
       "</g>\n",
       "<!-- 研究する&#45;&gt;計算機科学 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>研究する&#45;&gt;計算機科学</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459.1,-359.7C459.1,-352.41 459.1,-343.73 459.1,-335.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.6,-335.62 459.1,-325.62 455.6,-335.62 462.6,-335.62\"/>\n",
       "</g>\n",
       "<!-- 知能を -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>知能を</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"507.1\" cy=\"-450\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"507.1\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">知能を</text>\n",
       "</g>\n",
       "<!-- 知能を&#45;&gt;研究する -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>知能を&#45;&gt;研究する</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M495.72,-432.41C490.18,-424.34 483.39,-414.43 477.17,-405.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"480.12,-403.47 471.58,-397.2 474.35,-407.43 480.12,-403.47\"/>\n",
       "</g>\n",
       "<!-- の -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>の</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459.1\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459.1\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">の</text>\n",
       "</g>\n",
       "<!-- 計算機科学&#45;&gt;の -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>計算機科学&#45;&gt;の</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459.1,-287.7C459.1,-280.41 459.1,-271.73 459.1,-263.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.6,-263.62 459.1,-253.62 455.6,-263.62 462.6,-263.62\"/>\n",
       "</g>\n",
       "<!-- 一分野を -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>一分野を</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459.1\" cy=\"-162\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459.1\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">一分野を</text>\n",
       "</g>\n",
       "<!-- の&#45;&gt;一分野を -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>の&#45;&gt;一分野を</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459.1,-215.7C459.1,-208.41 459.1,-199.73 459.1,-191.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.6,-191.62 459.1,-181.62 455.6,-191.62 462.6,-191.62\"/>\n",
       "</g>\n",
       "<!-- 指す -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>指す</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"459.1\" cy=\"-90\" rx=\"29.5\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"459.1\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">指す</text>\n",
       "</g>\n",
       "<!-- 一分野を&#45;&gt;指す -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>一分野を&#45;&gt;指す</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459.1,-143.7C459.1,-136.41 459.1,-127.73 459.1,-119.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.6,-119.62 459.1,-109.62 455.6,-119.62 462.6,-119.62\"/>\n",
       "</g>\n",
       "<!-- 指す&#45;&gt;語 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>指す&#45;&gt;語</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M435.7,-78.46C430.59,-76.25 425.19,-73.99 420.1,-72 380.35,-56.43 333.92,-40.57 302.93,-30.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"304.29,-27.08 293.7,-27.27 302.1,-33.72 304.29,-27.08\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x10ce589d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "graph = Digraph(format=\"png\")\n",
    "for chunk in all_sentences_list[1]:\n",
    "    if chunk.has_dst():\n",
    "        graph.node(chunk.surface())\n",
    "        graph.node(chunk.to_dst_chunk())\n",
    "        graph.edge(chunk.surface(), chunk.to_dst_chunk())\n",
    "\n",
    "display(graph)#表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7053ba-56f9-4170-b145-17dae4459494",
   "metadata": {},
   "source": [
    "#45. 動詞の格パターンの抽出\n",
    "\n",
    "\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "・動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "・述語に係る助詞を格とする\n",
    "・述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "作り出す\tで は を\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "・コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "・「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f873a0c-79a8-4b67-ba81-96508b227017",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"work/case_corpus.txt\", \"w\") as fo:\n",
    "    for sent in all_sentences_list:\n",
    "        for chunk in sent:\n",
    "            if chunk.has_pos('動詞') and chunk.has_srcs():\n",
    "                srcs_chunk = []\n",
    "                for src_num in chunk.srcs:\n",
    "                    if chunk.sentence[src_num].has_pos(\"助詞\"):\n",
    "                        srcs_chunk.append(chunk.sentence[src_num].of_particle())\n",
    "                fo.write(f'{chunk.verb_base()}\\t{\" \".join(sorted(srcs_chunk))}\\n')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4c8b0c-96c9-4d9d-9f60-6659c4fc4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"work/case_corpus.txt\", \"w\") as fo:\n",
    "    for sent in all_sentences_list:\n",
    "        for chunk in sent:\n",
    "            if chunk.has_pos('動詞') and chunk.has_srcs():\n",
    "                srcs_chunk = []\n",
    "                for src_num in chunk.srcs:\n",
    "                    if chunk.sentence[src_num].morphs[-1].pos == \"助詞\":\n",
    "                        srcs_chunk.append(chunk.sentence[src_num].get_particle())\n",
    "                fo.write(f'{chunk.verb_base()}\\t{\" \".join(sorted(srcs_chunk))}\\n')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c93545c-a47f-456a-bdf9-fea7379e2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  59 する\tを\n",
      "  27 する\tと\n",
      "  27 する\tが\n",
      "  18 する\tに\n",
      "  12 する\tは を\n",
      "  11 する\tに を\n",
      "  10 する\tで を\n",
      "  10 よる\tに\n",
      "   9 行う\tを\n",
      "   7 する\tが に\n"
     ]
    }
   ],
   "source": [
    "!cat work/case_corpus.txt | sort | uniq -c | sort -nr | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b58adb5-1640-4bad-b48b-6b1dcb601b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9 行う\tを\n",
      "   4 なる\tに は\n",
      "   4 なる\tが と\n",
      "   2 行う\tに を\n",
      "   2 なる\tに\n",
      "   1 与える\tが などに\n",
      "   1 与える\tに は を\n",
      "   1 与える\tが に\n",
      "   1 なる\tとして に は\n",
      "   1 なる\tから が て では と\n"
     ]
    }
   ],
   "source": [
    "!cat work/case_corpus.txt | grep -E '^行う|^なる|^与える' | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f92aa-9ff7-4d69-89cf-75d670abf52d",
   "metadata": {},
   "source": [
    "#46. 動詞の格フレーム情報の抽出\n",
    "\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "作り出す\tで は を\t会議で ジョンマッカーシーは 用語を\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d328366-23b4-4f76-904a-5cfb838d2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#　　同順にするためにタプルにしてみる\n",
    "with open(\"work/case_corpus.txt\", \"w\") as fo:\n",
    "    for sent in all_sentences_list:\n",
    "        for chunk in sent:\n",
    "            if chunk.has_pos('動詞') and chunk.has_srcs():\n",
    "                case = []\n",
    "                for src_num in chunk.srcs:\n",
    "                    if chunk.sentence[src_num].morphs[-1].pos == \"助詞\":\n",
    "                        info = (chunk.sentence[src_num].get_particle(), chunk.sentence[src_num].surface())\n",
    "                        case.append(info)\n",
    "                if case:\n",
    "                    case.sort(key = lambda x: x[0])\n",
    "                    fo.write(f'{chunk.verb_base()}\\t{\" \".join([k[0] for k in case])}\\t{\" \".join([v[1] for v in case])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec7293d-a74a-4124-99dc-7a688e167195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる\tを\t道具を\n",
      "する\tて を\t用いて 知能を\n",
      "指す\tを\t一分野を\n",
      "代わる\tに を\t人間に 知的行動を\n",
      "行う\tて に\t代わって コンピューターに\n",
      "する\tとも\t研究分野とも\n",
      "述べる\tに は\t次のように 佐藤理史は\n",
      "する\tで を\tコンピュータ上で 知的能力を\n",
      "する\tを\t推論判断を\n",
      "する\tを\t画像データを\n"
     ]
    }
   ],
   "source": [
    "!head \"work/case_corpus.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e33250-37f2-43da-96ad-e9b65c59e28c",
   "metadata": {},
   "source": [
    "#47. 機能動詞構文のマイニング\n",
    "\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "学習を行う\tに を\t元に 経験を\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ff0965-5e37-4bdb-b2e7-912233e4322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "記述をする\tと\t主体と\n",
      "注目を集める\tが\tサポートベクターマシンが\n",
      "経験を行う\tに を\t元に 学習を\n",
      "学習を行う\tに に を を\t元に 元に 学習を 経験を\n",
      "進化を見せる\tにおいて\t生成技術において\n",
      "開発を行う\tは\tエイダ・ラブレスは\n",
      "意味をする\tに\tデータに\n",
      "研究を進める\tて\t費やして\n",
      "命令をする\tで\t機構で\n",
      "運転をする\tに\t元に\n",
      "特許をする\tが までに\t日本が 2018年までに\n",
      "運転をする\tに\t柔軟に\n",
      "注目を集める\tは\tファジィは\n",
      "研究を続ける\tて\t向けて\n",
      "注目を集める\tに\t急速に\n",
      "投資を行う\tで に\t民間企業主導で 全世界的に\n",
      "探索を行う\tで\t無報酬で\n",
      "推論をする\tて\t経て\n",
      "投資をする\tまでに\t2022年までに\n",
      "反乱を起こす\tに対して\t人間に対して\n",
      "監視を行う\tに まで\t人工知能に 歩行者まで\n",
      "判断を介す\tから\t観点から\n",
      "禁止を求める\tには\t4月には\n",
      "追及を受ける\tで と\t整合性で 拒否すると\n",
      "解任をする\tは\tGoogle社員らは\n",
      "解散をする\tが は\t倫理委員会が Googleは\n",
      "話をする\tは\t哲学者は\n",
      "議論を行う\tで は まで\t対談で 須藤は これまで\n"
     ]
    }
   ],
   "source": [
    "for sent in all_sentences_list:\n",
    "    for chunk in sent:\n",
    "        if chunk.has_pos('動詞'):\n",
    "            verb = chunk.verb_base()\n",
    "            case = []\n",
    "            for src_num in chunk.srcs:\n",
    "                if len(chunk.sentence[src_num].morphs) >= 2 and chunk.sentence[src_num].morphs[0].pos1 == 'サ変接続' and chunk.sentence[src_num].morphs[1].surface == 'を':\n",
    "                    #print(chunk.sentence[src_num].surface()+verb)\n",
    "                    for src_id in chunk.srcs:\n",
    "                        if src_id == src_num:\n",
    "                            continue\n",
    "                        elif chunk.sentence[src_id].morphs[-1].pos == \"助詞\":\n",
    "                            info = (chunk.sentence[src_id].get_particle(), chunk.sentence[src_id].surface())\n",
    "                            case.append(info)\n",
    "                    if case:\n",
    "                        case.sort(key = lambda x: x[0])\n",
    "                        print(f'{chunk.sentence[src_num].surface()+verb}\\t{\" \".join([k[0] for k in case])}\\t{\" \".join([v[1] for v in case])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c8f54-48db-4fb5-9973-7126aff4d9b7",
   "metadata": {},
   "source": [
    "#48. 名詞から根へのパスの抽出\n",
    "\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "各文節は（表層形の）形態素列で表現する\n",
    "パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "\n",
    "ジョンマッカーシーは -> 作り出した\n",
    "AIに関する -> 最初の -> 会議で -> 作り出した\n",
    "最初の -> 会議で -> 作り出した\n",
    "会議で -> 作り出した\n",
    "人工知能という -> 用語を -> 作り出した\n",
    "用語を -> 作り出した\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81ee90dd-f013-480b-9e7f-015e2b8aa9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジョンマッカーシーは->作り出した\n",
      "AIに関する->最初の->会議で->作り出した\n",
      "最初の->会議で->作り出した\n",
      "会議で->作り出した\n",
      "人工知能という->用語を->作り出した\n",
      "用語を->作り出した\n"
     ]
    }
   ],
   "source": [
    "sent = all_sentences_list[33]\n",
    "conect = []\n",
    " \n",
    "for chunk in sent:\n",
    "    if chunk.has_pos('名詞') and chunk.dst != -1:\n",
    "        conect.append(chunk.surface())\n",
    "        current_c = chunk\n",
    "        while current_c.dst != -1:\n",
    "            conect.append(sent[current_c.dst].surface())\n",
    "            current_c = sent[current_c.dst]\n",
    "        print(\"->\".join(conect))\n",
    "        conect =[]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b638a6-72f4-4e8a-8243-c36817045943",
   "metadata": {},
   "source": [
    "#  49. 名詞間の係り受けパスの抽出\n",
    "\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\n",
    "とj\n",
    "（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "文節iから構文木の根に至る経路上に文節jが存在する場合: \n",
    "文節iから文節jのパスを表示\n",
    "上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合:\n",
    "文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "\n",
    "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
    "Xは | Yの -> 会議で | 作り出した\n",
    "Xは | Yで | 作り出した\n",
    "Xは | Yという -> 用語を | 作り出した\n",
    "Xは | Yを | 作り出した\n",
    "Xに関する -> Yの\n",
    "Xに関する -> 最初の -> Yで\n",
    "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
    "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
    "Xの -> Yで\n",
    "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
    "Xの -> 会議で | Yを | 作り出した\n",
    "Xで | Yという -> 用語を | 作り出した\n",
    "Xで | Yを | 作り出した\n",
    "Xという -> Yを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "891f58a1-6df0-4f95-a582-ee9ae9e76153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#パターンは２つに分かれる\n",
    "#①[i -> a -> b -> j -> 根] であれば、 [i -> a -> b -> j]\n",
    "#②[i -> a -> k -> 根]、[j -> b -> k -> 根] であれば、[i -> a | j -> b | k]   i -> aと j -> b　で　k　行き着くというイメージ\n",
    "#戦略\n",
    "#とにかくまず名詞を持つ文節を発見→その文節番号のペアを作成（全通り）→ペアの名詞をXとYに置換→そのペアごとに上記の２パターンのうちどれかを実現する\n",
    "#１の時は片方の分節番号の持つchunkの係先がもう片方の文節番号になるまで処理を続ける\n",
    "#2の時は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "fbb443f7-d50c-45c8-b383-9aeea3690049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#名詞を含むchunkの名詞部分を特定の文字列に変換する関数\n",
    "\n",
    "def chunk_surface_change(chunk, post):\n",
    "    change_word = ''\n",
    "    for m in chunk.morphs:\n",
    "        if m.pos != '名詞' and m.pos != '記号':\n",
    "            change_word += m.surface\n",
    "            \n",
    "        elif m.pos == '記号':#記号はスルー　「ジョン・マッカーシー対策」\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            change_word += post\n",
    "            \n",
    "    return change_word\n",
    "                \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "c7878b05-9f80-4e38-8b4e-51dac31fc727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "②\tXは | Yに関する -> 最初の -> 会議で | 作り出した\n",
      "0 2\n",
      "②\tXは | Yの -> 会議で | 作り出した\n",
      "0 3\n",
      "②\tXは | Yで | 作り出した\n",
      "0 4\n",
      "②\tXは | Yという -> 用語を | 作り出した\n",
      "0 5\n",
      "②\tXは | Yを | 作り出した\n",
      "1 2\n",
      "①\tXに関する -> Yの\n",
      "1 3\n",
      "①\tXに関する -> 最初の -> Yで\n",
      "1 4\n",
      "②\tXに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
      "1 5\n",
      "②\tXに関する -> 最初の -> 会議で | Yを | 作り出した\n",
      "2 3\n",
      "①\tXの -> Yで\n",
      "2 4\n",
      "②\tXの -> 会議で | Yという -> 用語を | 作り出した\n",
      "2 5\n",
      "②\tXの -> 会議で | Yを | 作り出した\n",
      "3 4\n",
      "②\tXで | Yという -> 用語を | 作り出した\n",
      "3 5\n",
      "②\tXで | Yを | 作り出した\n",
      "4 5\n",
      "①\tXという -> Yを\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "import re\n",
    "\n",
    "sent = all_sentences_list[33]\n",
    "nouns = []#名詞を含む文節の文節番号のリスト\n",
    "\n",
    "for chunk in sent:\n",
    "    if chunk.has_pos('名詞'):\n",
    "        nouns.append(chunk.idx)#名詞を含む文節のidx番号を追加\n",
    "\n",
    "for i, j in combinations(nouns, 2):#名詞を含む文節のペアごとに一個ずつ取り出してパスを作成　しかもイテレータ\n",
    "    print(i, j)\n",
    "    i_path = []\n",
    "    j_path = []\n",
    "    while i != j:#どのみち係先が根まで行けばi=j\n",
    "        if i < j:\n",
    "            i_path.append(i)\n",
    "            i = chunk.sentence[i].dst\n",
    "        else:#パターン②用\n",
    "            j_path.append(j)\n",
    "            j = chunk.sentence[j].dst\n",
    "    #print(j_path)\n",
    "    \n",
    "    if len(j_path) == 0:#パターン①用\n",
    "        chunk_x = re.sub('X+', 'X', chunk_surface_change(chunk.sentence[i_path[0]],'X'))#スタートの文節iの名詞部分をXに置き換える\n",
    "        chunk_y = re.sub('Y+', 'Y', chunk_surface_change(chunk.sentence[i], 'Y'))#ゴールの文節jの名詞部分をXに置き換える\n",
    "        path_x_y = [chunk_x] + [chunk.sentence[n].surface() for n in i_path[1:]] + [chunk_y]#i_pathに溜め込んだ、jに行き着くまでの文節番号を全て表層化する\n",
    "        print(f'{\"①\"}\\t{\" -> \".join(path_x_y)}')\n",
    "    \n",
    "    else:#パターン②用\n",
    "        chunk_x = re.sub('X+', 'X', chunk_surface_change(chunk.sentence[i_path[0]],'X'))\n",
    "        chunk_y = re.sub('Y+', 'Y', chunk_surface_change(chunk.sentence[j_path[0]],'Y'))\n",
    "        \n",
    "        chunk_k = chunk.sentence[i].surface()#根\n",
    "        \n",
    "        path_x = [chunk_x] + [chunk.sentence[n].surface() for n in i_path[1:]]#[i -> a -> k -> 根]\n",
    "        path_y = [chunk_y] + [chunk.sentence[n].surface() for n in j_path[1:]]#[j -> b -> k -> 根]\n",
    "        print(f'{\"②\"}\\t{\" | \".join([\" -> \".join(path_x), \" -> \".join(path_y), chunk_k])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d281ede-4053-48ae-bee0-001bbe6420a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
